---
title: "Regression"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This article is used to benchmark the evaluations obtained by **renoir** for regression problems against external results.

## Data
Different public datasets already used in regression problems are considered:

* The relative CPU Performance data from Ein-Dor and Feldmesser [1987]
* The city-cycle fuel consumption data from the Carnegie Mellon University Statistics library

Data was retrieved from the [UC Irvine Machine Learning Repository website](https://archive-beta.ics.uci.edu).

## Setup

Firstly, we load `renoir` and other needed packages:

```{r setup, message=FALSE}
library(renoir)
library(plotly)
library(htmltools)
```

### Learning Methods
Now we retrieve the ids of all the supported learners supporting regression problems. A list of supported methods is available through the `?list_supported_learning_methods` function call.

```{r learning_methods}
#list methods
learning.methods = list_supported_learning_methods(x = "regression")

#print in table
knitr::kable(x = learning.methods)
```


We can extract the ids by selecting the `id` column.

```{r learning_method_ids}
#learning method ids
learning.methods.ids = learning.methods$id
```

From the table, we can also see the default hyperparameters of the methods. We create here our default values, and define a function to dispatch them depending on the `id` in input:

```{r setup_hyperparameter}
#get hyperparameters
get_hp = function(id){
  
  #Generalised Linear Models with Penalisation
  lambda = 10^seq(3, -2, length=100)
  # alpha = seq(0.1, 0.9, length = 9)
  alpha = seq(0.1, 0.9, length = 5)
  gamma = c(0, 0.25, 0.5, 0.75, 1)
  
  #Random Forest
  ntree = c(10, 50, 100, 250, 500)
  
  #Generalised Boosted Regression Modelling
  eta = c(0.3, 0.1, 0.01, 0.001)

  #Support Vector Machines
  cost      = 2^seq(from = -5, to = 15, length.out = 5)
  svm.gamma = 2^seq(from = -15, to = 3, length.out = 4)
  degree    = seq(from = 1, to = 3, length.out = 3)
  nu        = seq(from = 0.1, to = 0.6, length.out = 6)
  
  #kNN
  k = seq(from = 1, to = 9, length.out = 5)

  #hyperparameters
  out = switch(
    id,
    'lasso'              = list(lambda = lambda),
    'ridge'              = list(lambda = lambda),
    'elasticnet'         = list(lambda = lambda, alpha = alpha),
    'relaxed_lasso'      = list(lambda = lambda, gamma = gamma),
    'relaxed_ridge'      = list(lambda = lambda, gamma = gamma),
    'relaxed_elasticnet' = list(lambda = lambda, gamma = gamma, alpha = alpha),
    'randomForest'       = list(ntree = ntree),
    'gbm'                = list(eta = eta, ntree = ntree),
    'linear_SVM'         = list(cost = cost),
    'polynomial_SVM'     = list(cost = cost, gamma = svm.gamma, degree = degree),
    'radial_SVM'         = list(cost = cost, gamma = svm.gamma),
    'sigmoid_SVM'        = list(cost = cost, gamma = svm.gamma),
    'linear_NuSVM'       = list(nu = nu),
    'polynomial_NuSVM'   = list(nu = nu, gamma = svm.gamma, degree = degree),
    'radial_NuSVM'       = list(nu = nu, gamma = svm.gamma),
    'sigmoid_NuSVM'      = list(nu = nu, gamma = svm.gamma),
    'gknn'               = list(k = k)
  )
  
  return(out)
}
```

### Performance metrics
A list of supported scoring metrics is available through the `?list_supported_performance_metrics` function call.

```{r performance_metrics_for_binomial_response}
#list metrics
performance.metrics = list_supported_performance_metrics(resp.type = "gaussian")

#print in table
knitr::kable(x = performance.metrics)
```

For this benchmark we want to select mean absolute error and mean squared error.

```{r performance_metrics_ids}
#metric for tuning
performance.metric.id.tuning = "mse"

#metrics for evaluation
performance.metric.ids.evaluation = c("mae", "mse")

```


### Sampling Methods

A list of supported sampling methods is available through the `?list_supported_sampling_methods` function call.

```{r sampling_methods}
#list methods
sampling.methods = list_supported_sampling_methods()

#print in table
knitr::kable(x = sampling.methods)
```

We decided to select the common scenario of a stratified 10-fold cross-validation for the tuning of hyperparameters, and repeated random sampling for the evaluation of the methodology.

```{r sampling_method_ids}
#sampling for tuning
sampling.method.id.tuning = "cv"

#sampling for evaluation
sampling.method.id.evaluation = "random"
```


## Benchmark
We can now run our analyses.

### CPU Performance Data
The relative CPU Performance data was retrieved from the [UC Irvine Machine Learning Repository website](https://archive-beta.ics.uci.edu/ml/datasets/computer+hardware).

#### Load data
We load the data.

```{r load_cpu}
#load data
load(file = file.path("..", "..", "data-raw", "benchmark", "regression", "gaussian", "cpu", "data", "cpu_data.rda"))

#set response type
resp.type = "gaussian"
```

#### Setup

##### Tuner
Now we can create a tuner performing a `grid.search` via 10-fold cross-validation.

```{r setup_tuner_cpu}
#tuner
tuner = Tuner(
  id = "grid.search",
  sampler = Sampler(
    method = sampling.method.id.tuning,
    k = 10L,
    n = integer()
  ),
  looper   = Looper(cores = 1L),
  logger   = Logger(verbose = T, level = "INFO")
)
```

##### Learner
We can now create the related `?Learner` objects.

```{r setup_learners_cpu}
#container
learners = list()

#loop
for(learning.method.id in learning.methods.ids){
  #manual setup
  learners[[learning.method.id]] = Learner(
    tuner      = tuner,
    trainer    = Trainer(id = learning.method.id),
    forecaster = Forecaster(id = learning.method.id),
    scorer     = ScorerList(Scorer(id = performance.metric.id.tuning)),
    selector   = Selector(id = learning.method.id),
    recorder   = Recorder(id = learning.method.id, logger = Logger(level = "ALL", verbose = T)),
    marker     = Marker(id = learning.method.id, logger = Logger(level = "ALL", verbose = T)),
    logger     = Logger(level = "ALL")
  )
}
```

##### Evaluator
Finally, we need to set up the `?Evaluator`.

```{r setup_evaluator_cpu}
#Evaluator
evaluator = Evaluator(
  #Sampling strategy: random sampling without replacement
  sampler = Sampler(               
    method = "random",             
    k = 10L,                       
    N = as.integer(length(y))      
  ),

  #Performance metric
  scorer  = ScorerList(
    Scorer(id = performance.metric.ids.evaluation[1]),
    Scorer(id = performance.metric.ids.evaluation[2])
  )
)
```

#### Analysis
Let's create a directory to store the results.

```{r cpu_dir_setup, eval=T, include=T}
#define path
outdir = file.path("..", "..", "data-raw", "benchmark", "regression", "gaussian", "cpu", "analysis")

#create if not existing
if(!dir.exists(outdir)){dir.create(path = outdir, showWarnings = F, recursive = T)}
```

Before running the analysis, we want to set a seed for the random number generation (RNG). In fact, different R sessions have different seeds created from current time and process ID by default, and consequently different simulation results. By fixing a seed we ensure we will be able to reproduce the results. We can specify a seed by calling `?set.seed`.

In the code below, we set a seed before running the analysis for each considered learning method. 

```{r renoir_cpu, eval=FALSE}
#container list
resl = list()

#loop
for(learning.method.id in learning.methods.ids){
  
  #Each analysis can take hours, so we save data 
  #for future faster load
  
  #path to file
  fp.obj = file.path(outdir, paste0(learning.method.id,".rds"))
  fp.sum = file.path(outdir, paste0("st_",learning.method.id,".rds"))
  
  #check if exists
  if(file.exists(fp.sum)){
    #load
    cat(paste0("Reading ", learning.method.id, "..."), sep = "")
    resl[[learning.method.id]] = readRDS(file = fp.sum)
    cat("DONE", sep = "\n")
  } else {
  
    cat(paste("Learning method:", learning.method.id), sep = "\n")
    
    #Set a seed for RNG
    set.seed(
      #A seed
      seed = 5381L,                   #a randomly chosen integer value
      #The kind of RNG to use
      kind = "Mersenne-Twister",      #we make explicit the current R default value
      #The kind of Normal generation
      normal.kind = "Inversion"       #we make explicit the current R default value
    )
    
    resl[[learning.method.id]] = renoir(
      # filter,
  
      #Training set size
      npoints = 5,
      # ngrid,
      nmin = round(nrow(x)/2),
  
      #Loop
      looper = Looper(),
  
      #Store
      filename = "renoir",
      outdir   = NULL,
      restore  = TRUE,
  
      #Learn
      learner   = learners[[learning.method.id]],
  
      #Evaluate
      evaluator = evaluator,
  
      #Log
      logger    = Logger(level = "ALL", verbose = T),
  
      #Data for training
      hyperparameters = get_hp(id = learning.method.id),
      x         = x,
      y         = y,
      weights   = NULL,
      offset    = NULL,
      resp.type = resp.type,
  
      #Free space
      rm.call = FALSE,
      rm.fit  = FALSE,
  
      #Group results
      grouping = TRUE,
  
      #No screening
      screening = NULL,
      
      #Remove call from trainer to reduce space
      keep.call = F
    )
    
    #save obj
    saveRDS(object = resl[[learning.method.id]], file = fp.obj)
    
    #create summary table
    resl[[learning.method.id]] = renoir:::summary_table.RenoirList(resl[[learning.method.id]], key = c("id", "config"))
    
    #save summary table
    saveRDS(object = resl[[learning.method.id]], file = fp.sum)
    
    #leave space
    cat("\n\n", sep= "\n")
  }
}

#create summary table
resl = do.call(what = rbind, args = c(resl, make.row.names = F, stringsAsFactors = F))
```


#### Performance
Let's now plot the performance metrics for the `opt` and `1se` configurations, considering the train, test, and full set of data.

```{r cpu_plot_dir_setup, eval=T, include=FALSE}
#define path
outdir = file.path("..", "..", "data-raw", "benchmark", "regression", "gaussian", "cpu", "analysis", "plots")

#create if not existing
if(!dir.exists(outdir)){dir.create(path = outdir, showWarnings = F, recursive = T)}
```

##### Mean Absolute Error
We consider the mean absolute error.

###### Reference
This is the mean error on unseen data reported in Quinlan (1993) that we use as reference.

```{r include_cpu_mean_error_plot, echo=FALSE, fig.align='center', fig.cap='', out.width='90%'}
knitr::include_graphics(path = file.path("..", "..", "man", "figures", "cpu_me_test.png"), error = FALSE)
```

###### Train
This is the score for the `opt` configuration when considering the train set.

```{r cpu_plot_mae_multi_opt_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mae", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r cpu_plot_mae_multi_opt_train, eval=TRUE, include=FALSE, echo = TRUE, results = 'asis'}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_opt_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mae",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mae", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```


```{r cpu_print_plot_mae_multi_opt_train2, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the score for the `1se` configuration when considering the train set.

```{r cpu_plot_mae_multi_1se_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mae", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r cpu_plot_mae_multi_1se_train, eval=TRUE, include=FALSE, echo = TRUE, results = 'asis'}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_1se_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F], 
    measure = "mae", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
htmltools::tagList(p2)
```

```{r cpu_print_plot_mae_multi_1se_train, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Test
This is the mae score for the `opt` configuration when considering the test set.

```{r cpu_plot_mae_multi_opt_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mae", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r cpu_plot_mae_multi_opt_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_opt_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mae",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mae", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mae_multi_opt_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mae score for the `1se` configuration when considering the test set.

```{r cpu_plot_mae_multi_1se_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mae", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r cpu_plot_mae_multi_1se_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_1se_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mae_multi_1se_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Full

This is the mae score for the `opt` configuration when considering the full set.

```{r cpu_plot_mae_multi_opt_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mae", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r cpu_plot_mae_multi_opt_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_opt_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mae",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mae", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mae_multi_opt_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mae score for the `1se` configuration when considering the full set.

```{r cpu_plot_mae_multi_1se_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mae", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r cpu_plot_mae_multi_1se_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_1se_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F], 
    measure = "mae", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mae_multi_1se_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

##### Mean Squared Error
We consider here the mean squared error.

###### Train
This is the mse score for the `opt` configuration when considering the train set.

```{r cpu_plot_mse_multi_opt_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mse", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r cpu_plot_mse_multi_opt_train, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_opt_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 =renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mse", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mse_multi_opt_train, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mse score for the `1se` configuration when considering the train set.

```{r cpu_plot_mse_multi_1se_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mse", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r cpu_plot_mse_multi_1se_train, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_1se_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F], 
    measure = "mse", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mse_multi_1se_train, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Test
This is the mse score for the `opt` configuration when considering the test set.

```{r cpu_plot_mse_multi_opt_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mse", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r cpu_plot_mse_multi_opt_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_opt_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mse", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mse_multi_opt_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mse score for the `1se` configuration when considering the test set.

```{r cpu_plot_mse_multi_1se_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mse", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r cpu_plot_mse_multi_1se_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_1se_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mse_multi_1se_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Full

This is the mse score for the `opt` configuration when considering the full set.

```{r cpu_plot_mse_multi_opt_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mse", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r cpu_plot_mse_multi_opt_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_opt_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mse", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mse_multi_opt_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mse score for the `1se` configuration when considering the full set.

```{r cpu_plot_mse_multi_1se_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mse", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r cpu_plot_mse_multi_1se_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_1se_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r cpu_print_plot_mse_multi_1se_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```


### Auto MPG Data
The city-cycle fuel consumption data was retrieved from the [UC Irvine Machine Learning Repository website](https://archive-beta.ics.uci.edu/ml/datasets/auto+mpg).

#### Load data
We load the data.

```{r load_autompg}
#load data
load(file = file.path("..", "..", "data-raw", "benchmark", "regression", "gaussian", "auto_mpg", "data", "autompg_data.rda"))

#set response type
resp.type = "gaussian"
```

#### Setup

##### Tuner
Now we can create a tuner performing a `grid.search` via 10-fold cross-validation.

```{r setup_tuner_autompg}
#tuner
tuner = Tuner(
  id = "grid.search",
  sampler = Sampler(
    method = sampling.method.id.tuning,
    k = 10L,
    n = integer()
  ),
  looper   = Looper(cores = 1L),
  logger   = Logger(verbose = T, level = "INFO")
)
```

##### Learner
We can now create the related `?Learner` objects.

```{r setup_learners_autompg}
#container
learners = list()

#loop
for(learning.method.id in learning.methods.ids){
  #manual setup
  learners[[learning.method.id]] = Learner(
    tuner      = tuner,
    trainer    = Trainer(id = learning.method.id),
    forecaster = Forecaster(id = learning.method.id),
    scorer     = ScorerList(Scorer(id = performance.metric.id.tuning)),
    selector   = Selector(id = learning.method.id),
    recorder   = Recorder(id = learning.method.id, logger = Logger(level = "ALL", verbose = T)),
    marker     = Marker(id = learning.method.id, logger = Logger(level = "ALL", verbose = T)),
    logger     = Logger(level = "ALL")
  )
}
```

##### Evaluator
Finally, we need to set up the `?Evaluator`.

```{r setup_evaluator_autompg}
#Evaluator
evaluator = Evaluator(
  #Sampling strategy: random sampling without replacement
  sampler = Sampler(               
    method = "random",             
    k = 10L,                       
    N = as.integer(length(y))      
  ),

  #Performance metric
  scorer  = ScorerList(
    Scorer(id = performance.metric.ids.evaluation[1]),
    Scorer(id = performance.metric.ids.evaluation[2])
  )
)
```

#### Analysis
Let's create a directory to store the results.

```{r autompg_dir_setup, eval=T, include=T}
#define path
outdir = file.path("..", "..", "data-raw", "benchmark", "regression", "gaussian", "auto_mpg", "analysis")

#create if not existing
if(!dir.exists(outdir)){dir.create(path = outdir, showWarnings = F, recursive = T)}
```

Before running the analysis, we want to set a seed for the random number generation (RNG). In fact, different R sessions have different seeds created from current time and process ID by default, and consequently different simulation results. By fixing a seed we ensure we will be able to reproduce the results. We can specify a seed by calling `?set.seed`.

In the code below, we set a seed before running the analysis for each considered learning method. 

```{r renoir_autompg, eval=FALSE}
#container list
resl = list()

#loop
for(learning.method.id in learning.methods.ids[c(11,12,15,16,17,13,9)]){
  
  #Each analysis can take hours, so we save data 
  #for future faster load
  
  #path to file
  fp.obj = file.path(outdir, paste0(learning.method.id,".rds"))
  fp.sum = file.path(outdir, paste0("st_",learning.method.id,".rds"))
  
  #check if exists
  if(file.exists(fp.sum)){
    #load
    cat(paste0("Reading ", learning.method.id, "..."), sep = "")
    resl[[learning.method.id]] = readRDS(file = fp.sum)
    cat("DONE", sep = "\n")
  } else {
  
    cat(paste("Learning method:", learning.method.id), sep = "\n")
    
    #Set a seed for RNG
    set.seed(
      #A seed
      seed = 5381L,                   #a randomly chosen integer value
      #The kind of RNG to use
      kind = "Mersenne-Twister",      #we make explicit the current R default value
      #The kind of Normal generation
      normal.kind = "Inversion"       #we make explicit the current R default value
    )
    
    resl[[learning.method.id]] = renoir(
      # filter,
  
      #Training set size
      npoints = 5,
      # ngrid,
      nmin = round(nrow(x)/2),
  
      #Loop
      looper = Looper(),
  
      #Store
      filename = "renoir",
      outdir   = NULL,
      restore  = TRUE,
  
      #Learn
      learner   = learners[[learning.method.id]],
  
      #Evaluate
      evaluator = evaluator,
  
      #Log
      logger    = Logger(level = "ALL", verbose = T),
  
      #Data for training
      hyperparameters = get_hp(id = learning.method.id),
      x         = x,
      y         = y,
      weights   = NULL,
      offset    = NULL,
      resp.type = resp.type,
  
      #space
      rm.call = FALSE,
      rm.fit  = FALSE,
  
      #Group results
      grouping = TRUE,
  
      #No screening
      screening = NULL,
      
      #Remove call from trainer to reduce space
      keep.call = F
    )
    
    #save
    saveRDS(object = resl[[learning.method.id]], file = fp.obj)
    
    #create summary table
    resl[[learning.method.id]] = renoir:::summary_table.RenoirList(resl[[learning.method.id]], key = c("id", "config"))
    
    #save summary table
    saveRDS(object = resl[[learning.method.id]], file = fp.sum)
    
    #leave space
    cat("\n\n", sep= "\n")
  }
}

#create summary table
resl = do.call(what = rbind, args = c(resl, make.row.names = F, stringsAsFactors = F))
```


#### Performance
Let's now plot the performance metrics for the `opt` and `1se` configurations, considering the train, test, and full set of data.

```{r autompg_plot_dir_setup, eval=T, include=FALSE}
#define path
outdir = file.path("..", "..", "data-raw", "benchmark", "regression", "gaussian", "auto_mpg", "analysis", "plots")

#create if not existing
if(!dir.exists(outdir)){dir.create(path = outdir, showWarnings = F, recursive = T)}
```

##### Mean Absolute Error
We consider the mean absolute error.

###### Reference
This is the mean error on unseen data reported in Quinlan (1993) that we use as reference.

```{r include_autompg_mean_error_plot, echo=FALSE, fig.align='center', fig.cap='', out.width='90%'}
knitr::include_graphics(path = file.path("..", "..", "man", "figures", "autompg_me_test.png"), error = FALSE)
```


###### Train
This is the score for the `opt` configuration when considering the train set.

```{r autompg_plot_mae_multi_opt_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mae", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r autompg_plot_mae_multi_opt_train, eval=TRUE, include=FALSE, echo = TRUE, results = 'asis'}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_opt_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mae",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mae", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```


```{r autompg_print_plot_mae_multi_opt_train2, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the score for the `1se` configuration when considering the train set.

```{r autompg_plot_mae_multi_1se_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mae", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r autompg_plot_mae_multi_1se_train, eval=TRUE, include=FALSE, echo = TRUE, results = 'asis'}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_1se_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
htmltools::tagList(p2)
```

```{r autompg_print_plot_mae_multi_1se_train, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Test
This is the mae score for the `opt` configuration when considering the test set.

```{r autompg_plot_mae_multi_opt_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mae", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r autompg_plot_mae_multi_opt_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_opt_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mae",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mae", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mae_multi_opt_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mae score for the `1se` configuration when considering the test set.

```{r autompg_plot_mae_multi_1se_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mae", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r autompg_plot_mae_multi_1se_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_1se_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F], 
    measure = "mae", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mae_multi_1se_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Full

This is the mae score for the `opt` configuration when considering the full set.

```{r autompg_plot_mae_multi_opt_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mae", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r autompg_plot_mae_multi_opt_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_opt_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mae",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mae", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mae_multi_opt_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mae score for the `1se` configuration when considering the full set.

```{r autompg_plot_mae_multi_1se_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mae", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r autompg_plot_mae_multi_1se_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mae_1se_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mae", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mae_multi_1se_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

##### Mean Squared Error
We consider here the mean squared error.

###### Train
This is the mse score for the `opt` configuration when considering the train set.

```{r autompg_plot_mse_multi_opt_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mse", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r autompg_plot_mse_multi_opt_train, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_opt_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F], 
    measure = "mse", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mse_multi_opt_train, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mse score for the `1se` configuration when considering the train set.

```{r autompg_plot_mse_multi_1se_train_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mse", 
  set         = "train", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r autompg_plot_mse_multi_1se_train, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_1se_train.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse",
    set = "train",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse", 
    set = "train", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mse_multi_1se_train, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Test
This is the mse score for the `opt` configuration when considering the test set.

```{r autompg_plot_mse_multi_opt_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mse", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r autompg_plot_mse_multi_opt_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_opt_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mse_multi_opt_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mse score for the `1se` configuration when considering the test set.

```{r autompg_plot_mse_multi_1se_test_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mse", 
  set         = "test", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r autompg_plot_mse_multi_1se_test, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_1se_test.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse",
    set = "test",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse", 
    set = "test", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mse_multi_1se_test, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

###### Full

This is the mse score for the `opt` configuration when considering the full set.

```{r autompg_plot_mse_multi_opt_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "opt",,drop=F], #select opt config
  measure     = "mse", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```


```{r autompg_plot_mse_multi_opt_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_opt_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  # 
  # #store
  # saveRDS(object = p, file = fp)
  
  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "opt",,drop=F],
    measure = "mse", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mse_multi_opt_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```

This is the mse score for the `1se` configuration when considering the full set.

```{r autompg_plot_mse_multi_1se_full_code_to_show, eval=FALSE}
#plot
renoir:::plot.RenoirSummaryTable(
  x = resl[resl$config == "1se",,drop=F], #select 1se config 
  measure     = "mse", 
  set         = "full", 
  interactive = T, 
  add.boxplot = F,
  add.scores  = F,
  add.best    = F,
  key         = c("id", "config")
  )
```

```{r autompg_plot_mse_multi_1se_full, eval=TRUE, include=FALSE, echo = TRUE}
#As we had issues in loading the results with readRDS (probably because file was too big) we decided to store the plot object and load that

fp = file.path(outdir, "plot_mse_1se_full.rda")

if(file.exists(fp)){
  # p1 = readRDS(file = fp)
  load(file = fp)
} else {
  #plot
  p1 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse",
    set = "full",
    interactive = F,
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )

  p2 = renoir:::plot.RenoirSummaryTable(
    x = resl[resl$config == "1se",,drop=F],
    measure = "mse", 
    set = "full", 
    interactive = T, 
    add.boxplot = F,
    add.scores = F,
    add.best = F,
    key = c("id", "config")
  )
  
  save(p1, p2, file = fp)
  
}

#plot
print(htmltools::tagList(p2))
```

```{r autompg_print_plot_mse_multi_1se_full, eval=TRUE, results = 'asis', echo = FALSE}
htmltools::tagList(p2)
```
