% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/performance.R
\name{accuracy_score}
\alias{accuracy_score}
\title{Accuracy Classification Score}
\usage{
accuracy_score(true, pred, weights = NULL, multi = c("average", "raw"), ...)
}
\arguments{
\item{true}{a vector of observed values}

\item{pred}{a vector of predicted values}

\item{weights}{vector of observation weights}

\item{multi}{what to do when response has multiple classes
\describe{
  \item{\code{average}}{errors of multiple classes are averaged to get a single value}
  \item{\code{raw}}{returns a vector containing one score for each class}
}}

\item{...}{not currently used}
}
\value{
A numeric vector of length one if \code{multi = "average"} or \code{nc} if
\code{multi = "raw"}, where \code{nc} is the number of classes.
}
\description{
This function computes the accuracy of a classification.
}
\details{
The accuracy measures the fraction of all instances that are correctly categorized.
It is defined as:

\deqn{ACC = \frac{correct}{total} = \frac{TP + TN}{P + N} = \frac{TP + TN}{TP + FP + TN + FN} = 1 - ERR}

The optimal value is 1 and the worst value is 0.
The complementary statistic is the \code{\link[classification_error_rate]{classification error rate}}.
}
\references{
\url{https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers#Single_metrics}
}
\author{
Alessandro Barberis
}
