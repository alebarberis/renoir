% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/performance.R
\name{classification_error_rate}
\alias{classification_error_rate}
\title{Classification Error Rate}
\usage{
classification_error_rate(
  true,
  pred,
  weights = NULL,
  multi = c("average", "raw"),
  ...
)
}
\arguments{
\item{true}{a vector of observed values}

\item{pred}{a vector of predicted values}

\item{weights}{vector of observation weights}

\item{multi}{what to do when response has multiple classes
\describe{
\item{\code{average}}{errors of multiple classes are averaged to get a single value}
\item{\code{raw}}{returns a vector containing one score for each class}
}}

\item{...}{not currently used}
}
\value{
A numeric vector of length one if \code{multi = "average"} or \code{nc} if
\code{multi = "raw"}, where \code{nc} is the number of classes.
}
\description{
This function computes the misclassification error.
}
\details{
The classification error rate measures the fraction of all instances that are wrongly categorized.
It is defined as:

\deqn{ERR = \frac{errors}{total} = \frac{FP + FN}{P + N} = \frac{FP + FN}{TP + FP + TN + FN} = 1 - ACC}

The optimal value is 0 and the worst value is 1. The complementary statistic is the \code{\link[accuracy_score]{accuracy}}.
}
\references{
\url{https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers#Single_metrics}
}
\author{
Alessandro Barberis
}
